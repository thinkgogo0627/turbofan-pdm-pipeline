import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import ReduceLROnPlateau
import mlflow
import mlflow.pytorch
import mlflow.data # <--- Dataset ë¡œê¹…ìš© import
from mlflow.data.pandas_dataset import PandasDataset # <--- ëª…ì‹œì  import
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from numpy.lib.stride_tricks import sliding_window_view


PROJECT_DIR = Path(__file__).resolve().parents[2]

# ë¶„ë¦¬í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from src.features.schema import FeatureSchema
from src.models.model_zoo import DeepCNN, CNNAttention, TransformerModel, DLinear, Simple1DCNN
from src.models.model_config import TRAINER_CONFIG, MODEL_CONFIGS




def get_model(model_name, input_dim, model_conf):
    """ëª¨ë¸ ì´ë¦„ê³¼ ì„¤ì •ê°’ì„ ë°›ì•„ì„œ ê°ì²´ë¥¼ ìƒì„±í•´ì£¼ëŠ” Factory í•¨ìˆ˜"""
    
    if model_name == "DLinear":
        return DLinear(seq_len=model_conf['window_size'], input_dim=input_dim)
    
    elif model_name == "Transformer":
        return TransformerModel(input_dim=input_dim, d_model=model_conf['d_model'], nhead=model_conf['nhead'])
    
    elif model_name == "DeepCNN":
        return DeepCNN(input_dim=input_dim, hidden_layers=model_conf['hidden_layers'], kernel_size=model_conf['kernel_size'], dropout=model_conf['dropout'])
    
    elif model_name == "CNNAttention": # CNNAttentionë„ ì¶”ê°€
        return CNNAttention(input_dim=input_dim, hidden_dim=model_conf['hidden_dim'])
    
    else:
        return Simple1DCNN(input_dim=input_dim)



def train_model(model_name):
    # ---------------------------------------------------------
    # 1. Config í•©ì¹˜ê¸° (Merge Logic)
    # ---------------------------------------------------------
    if model_name not in MODEL_CONFIGS:
        # ëª¨ë¸ë³„ ì„¤ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¼ë„ ì‚¬ìš©í•˜ê±°ë‚˜ ì—ëŸ¬ ì²˜ë¦¬
        # ì—¬ê¸°ì„œëŠ” TRAINER_CONFIGë§Œ ì‚¬ìš©í•˜ë„ë¡ ì²˜ë¦¬í•  ìˆ˜ë„ ìˆìŒ
        print(f"âš ï¸ Warning: No specific config for {model_name}. Using default.")
        model_specific_conf = {}
    else:
        model_specific_conf = MODEL_CONFIGS[model_name]

    # [í•µì‹¬] ë‘ ë”•ì…”ë„ˆë¦¬ ë³‘í•© (.copy()ë¡œ ì›ë³¸ ì˜¤ì—¼ ë°©ì§€)
    # full_config = ê³µí†µ ì„¤ì • + ëª¨ë¸ë³„ ì„¤ì • + ëª¨ë¸ ì´ë¦„
    full_config = TRAINER_CONFIG.copy()
    full_config.update(model_specific_conf)
    full_config['model_type'] = model_name # ì´ë¦„ë„ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡
    
    # ---------------------------------------------------------
    # MLflow ì„¸íŒ…
    # ---------------------------------------------------------
    current_time = datetime.now().strftime("%m%d_%H%M")
    run_name = f"{model_name}_{current_time}"
    mlflow.set_experiment("Turbofan_RUL_Prediction")

    with mlflow.start_run(run_name=run_name):
        print(f"ğŸš€ Start Training: {model_name}")
        print(f"ğŸ“œ Full Config: {full_config}")
        
        # [í•µì‹¬] í•©ì³ì§„ ì„¤ì •ì„ ê¸°ë¡ -> ì´ì œ Window Size ë³´ì…ë‹ˆë‹¤!
        mlflow.log_params(full_config) 

        # ----------------------------------------
        # ë°ì´í„° ë¡œë“œ
        # ----------------------------------------
        data_path = PROJECT_DIR / "data/processed/train_FD001_features.parquet"
        df = pd.read_parquet(data_path)
        
        # ---------------------------------------------------------
        # [í•µì‹¬] Dataset ì •ë³´ MLflowì— ë“±ë¡ (Data Lineage)
        # ---------------------------------------------------------
        print("[Info] Logging dataset info to MLflow...")
        dataset = mlflow.data.from_pandas(
            df, 
            source=str(data_path), 
            name="turbofan_processed_data_ver_1"
        )
        mlflow.log_input(dataset, context="training")
        # ---------------------------------------------------------
        
        # Scaling
        scaler = MinMaxScaler()
        # ì£¼ì˜: configì— ìˆëŠ” features ë¦¬ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
        feature_cols = full_config['features']
        df[feature_cols] = scaler.fit_transform(df[feature_cols])
        
        # Windowing (merged configì—ì„œ window_size ê°€ì ¸ì˜´)
        X, y = create_dataset(df, full_config['window_size'], feature_cols)
        
        # DataLoader ìƒì„±
        dataset_tensor = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))
        dataloader = DataLoader(dataset_tensor, batch_size=full_config['batch_size'], shuffle=True)

        # ----------------------------------------
        # ëª¨ë¸ ì´ˆê¸°í™”
        # ----------------------------------------
        model = get_model(model_name, len(feature_cols), full_config)
        
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=full_config['learning_rate'])
        
        # Patienceë„ configì—ì„œ ê°€ì ¸ì˜¤ê¸°
        patience = full_config.get('patience', 10) 
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience)

        # ----------------------------------------
        # í•™ìŠµ ë£¨í”„
        # ----------------------------------------
        model.train()
        for epoch in range(full_config['epochs']):
            epoch_loss = 0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(dataloader)
            rmse = np.sqrt(avg_loss)
            
            # Scheduler Step
            scheduler.step(avg_loss)
            
            # Logging
            if (epoch+1) % 10 == 0:
                lr = optimizer.param_groups[0]['lr']
                print(f"Epoch {epoch+1}/{full_config['epochs']} | RMSE: {rmse:.4f} | LR: {lr:.6f}")
                mlflow.log_metric("rmse", rmse, step=epoch)
                mlflow.log_metric("learning_rate", lr, step=epoch)

        # ì €ì¥
        mlflow.pytorch.log_model(model, "model")
        print("ğŸ‰ Training Finished.")

def create_dataset(df, window_size, feature_cols):
    X_list, y_list = [], []
    
    print(f"[Info] Creating windows (Size: {window_size})...") # ì§„í–‰ìƒí™© ì¶œë ¥
    
    for unit_nr, group in df.groupby('unit_nr'):
        data = group[feature_cols].values
        target = group['RUL'].values
        
        if len(data) < window_size:
            continue
            
        # ğŸš€ [NumPy Magic] ë°˜ë³µë¬¸ ì—†ì´ í•œë°©ì— ìë¥´ê¸°
        # sliding_window_view: ë©”ëª¨ë¦¬ ë³µì‚¬ ì—†ì´ ë·°ë§Œ ìƒì„±í•´ì„œ ì—„ì²­ ë¹ ë¦„
        # shape: (num_windows, window_size, num_features)
        windows = sliding_window_view(data, window_shape=window_size, axis=0).transpose(0, 2, 1)
        
        # yê°’ì€ ê° ìœˆë„ìš°ì˜ 'ë§ˆì§€ë§‰ ì‹œì 'ì˜ RUL
        # target[window_size-1 :] ê³¼ ë™ì¼
        target_windows = target[window_size-1:]
        
        X_list.append(windows)
        y_list.append(target_windows)
        
    # ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    X = np.concatenate(X_list, axis=0)
    y = np.concatenate(y_list, axis=0)
    
    print(f"[Info] Windowing Complete! Shape: {X.shape}") # ì™„ë£Œ ë©”ì‹œì§€
    return X, y

if __name__ == "__main__":
    # ì›í•˜ëŠ” ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
    train_model("DeepCNN")