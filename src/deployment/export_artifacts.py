import sys
from pathlib import Path

# 1. ê²½ë¡œ ë¨¼ì € ì„¤ì •
PROJECT_DIR = Path(__file__).resolve().parents[2]

# 2. [ê°€ì¥ ì¤‘ìš”] íŒŒì´ì¬ì´ 'src' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ê²½ë¡œë¥¼ ê°•ì œë¡œ ì£¼ì…
sys.path.append(str(PROJECT_DIR))

import os
import ast
import json
import torch
import joblib
import mlflow
import pandas as pd
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA

# ê²½ë¡œ ì„¤ì •
PROJECT_DIR = Path(__file__).resolve().parents[2]
ARTIFACT_DIR = PROJECT_DIR / "app/artifacts"  # FastAPI ë„ì»¤ê°€ ì½ì„ í´ë”
os.makedirs(ARTIFACT_DIR, exist_ok=True)

# GPU/CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

## ê¸°ì¡´ ì•™ìƒë¸” ë¡œì§ ê·¸ëŒ€ë¡œ ì ìš©
def export_top_models(top_n=3):
    """
    [MLOps Architecture] Dynamic Metadata Extraction
    - ì˜¤ë¦¬ì§€ë„ ì•™ìƒë¸” ì½”ë“œì™€ 100% ë™ì¼í•˜ê²Œ 'val_rmse' ê¸°ì¤€ìœ¼ë¡œ Top 3ë¥¼ ì„ ë°œí•©ë‹ˆë‹¤.
    - ì„ ë°œëœ ëª¨ë¸ë“¤ì˜ ê³¼ê±° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ JSONì— ì €ì¥í•©ë‹ˆë‹¤.
    
    â–  ë„ì… ë°°ê²½ (Why we do this?):
        - ë¬¸ì œì : ê³¼ê±° í•™ìŠµëœ ëª¨ë¸(ì˜ˆ: 128ì°¨ì›, 2ë ˆì´ì–´)ì˜ ê°€ì¤‘ì¹˜(.pth)ë¥¼ 
                 í˜„ì¬ ì—…ë°ì´íŠ¸ëœ ì½”ë“œ(ì˜ˆ: 256ì°¨ì›, 4ë ˆì´ì–´)ì˜ ëª¨ë¸ ê»ë°ê¸°ì— ë®ì–´ì”Œìš°ë ¤ í•  ë•Œ 
                 ì°¨ì› ë¶ˆì¼ì¹˜(Shape Mismatch) ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” 'ì„¤ì • í‘œë¥˜(Configuration Drift)' í˜„ìƒ ë°œìƒ.
        - ì•ˆí‹° íŒ¨í„´: ë°°í¬ ì„œë²„(FastAPI) ê°œë°œìê°€ ì—ëŸ¬ ë¡œê·¸ë¥¼ ë³´ê³  í•˜ë“œì½”ë”©ìœ¼ë¡œ ìˆ«ìë¥¼ ë§ì¶°ì¤Œ. 
                   -> ëª¨ë¸ì´ ì¬í•™ìŠµë  ë•Œë§ˆë‹¤ ì„œë²„ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ì¹˜ëª…ì  ì˜ì¡´ì„± ë°œìƒ.

    â–  í•´ê²° ê¸°ëŠ¥ (What this does?):
        1. MLflowì—ì„œ ìˆœìˆ˜ ê°€ì¤‘ì¹˜(.pth)ë§Œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì´ ì•„ë‹˜.
        2. í•´ë‹¹ ê°€ì¤‘ì¹˜ê°€ í•™ìŠµë  ë‹¹ì‹œì— ì‚¬ìš©ë˜ì—ˆë˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°(d_model, nhead, num_layers ë“±)ë¥¼ 
           MLflow íŒŒë¼ë¯¸í„° ê¸°ë¡ì—ì„œ ë™ì ìœ¼ë¡œ í•¨ê»˜ ì¶”ì¶œ.
        3. ì´ ì •ë³´ë“¤ì„ 'ensemble_meta.json'ì´ë¼ëŠ” ì„¤ê³„ë„ íŒŒì¼ì— ë¬¶ì–´ì„œ ë°°í¬.
        4. ì„œë¹™ ì„œë²„ëŠ” ì´ JSONì„ ì½ê³  "ìŠ¤ìŠ¤ë¡œ ì•Œë§ì€ ê»ë°ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±"í•˜ê²Œ ë¨.
           -> ëª¨ë¸ êµ¬ì¡°ê°€ ì•„ë¬´ë¦¬ ë°”ë€Œì–´ë„ ì„œë¹™ ì½”ë“œëŠ” ë‹¨ í•œ ì¤„ë„ ìˆ˜ì •í•  í•„ìš” ì—†ëŠ” ì™„ì „ ìë™í™” ë‹¬ì„±.
    
    """
    print(f"ğŸš€ [MLOps] Extracting Top {top_n} Models (by Val RMSE) from MLflow...")
    
    mlflow.set_experiment("Turbofan_RUL_Prediction")
    experiment = mlflow.get_experiment_by_name("Turbofan_RUL_Prediction")
    
    # [ìˆ˜ì •ë¨] ì˜¤ë¦¬ì§€ë„ ì•™ìƒë¸” ë¡œì§ê³¼ ì™„ë²½í•˜ê²Œ ë™ì¼í•œ ì¿¼ë¦¬ (Val RMSE ì˜¤ë¦„ì°¨ìˆœ)
    runs = mlflow.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=["metrics.val_rmse ASC"], 
        max_results=top_n
    )
    
    ensemble_meta = {
        "models": [],
        "ensemble_method": "average",
        "expected_features": 9
    }
    
    for index, run in runs.iterrows():
        run_id = run.run_id
        model_name = run['params.model_type']
        window_size = int(run['params.window_size'])
        val_rmse = run['metrics.val_rmse']
        
        # [ì•ˆì „ì¥ì¹˜] DataFrameì— í•´ë‹¹ íŒŒë¼ë¯¸í„° ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜ NaNì¼ ê²½ìš° ëŒ€ë¹„ (ì—ëŸ¬ ë°©ì§€)
        d_model = int(run['params.d_model']) if 'params.d_model' in run and pd.notna(run['params.d_model']) else 128
        nhead = int(run['params.nhead']) if 'params.nhead' in run and pd.notna(run['params.nhead']) else 4
        num_layers = int(run['params.num_layers']) if 'params.num_layers' in run and pd.notna(run['params.num_layers']) else 2
        
        print(f"  -> Exporting Rank {index+1}: {model_name} (W={window_size}, Val RMSE: {val_rmse:.4f})")
        print(f"     [Metadata] d_model={d_model}, nhead={nhead}, layers={num_layers}")
        
        model = mlflow.pytorch.load_model(f"runs:/{run_id}/model").to(device)
        weight_filename = f"model_rank{index+1}_w{window_size}.pth"
        weight_path = ARTIFACT_DIR / weight_filename
        
        torch.save(model.state_dict(), weight_path)
        
        ensemble_meta["models"].append({
            "rank": index + 1,
            "filename": weight_filename,
            "model_type": model_name,
            "window_size": window_size,
            "val_rmse": val_rmse,
            "run_id": run_id,
            "hyperparams": {
                "d_model": d_model,
                "nhead": nhead,
                "num_layers": num_layers
            }
        })

    with open(ARTIFACT_DIR / "ensemble_meta.json", "w") as f:
        json.dump(ensemble_meta, f, indent=4)
        
    print(f"âœ… Models and metadata exported to {ARTIFACT_DIR}")
def export_preprocessors():
    print(f"ğŸš€ [MLOps] Regenerating and Exporting Preprocessors (Scaler, PCA)...")
    
    # í›ˆë ¨ ë°ì´í„° ë¡œë“œ (ì •í™•íˆ í›ˆë ¨ ë•Œ ì‚¬ìš©í•œ ê·¸ ë°ì´í„°)
    train_path = PROJECT_DIR / "data/processed/train_FD001_advanced_features.parquet"
    train_df = pd.read_parquet(train_path)
    
    # ì •ì˜ˆ 9ê°œ í”¼ì²˜ (í‰ê°€ ì½”ë“œì—ì„œ ê³ ì •í–ˆë˜ ê·¸ í”¼ì²˜ë“¤)
    # pca_1, pca_1_trend ìƒì„±ì„ ìœ„í•œ ì›ë³¸ ì„¼ì„œë“¤
    raw_sensors = ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_11', 'sensor_12', 'sensor_15']
    
    # 1. PCA ê°ì²´ ìƒì„± ë° ì €ì¥ (Fit)
    pca_scaler = StandardScaler()
    pca = PCA(n_components=1)
    
    train_scaled_for_pca = pca_scaler.fit_transform(train_df[raw_sensors])
    train_df['pca_1'] = pca.fit_transform(train_scaled_for_pca)
    train_df['pca_1_trend'] = train_df.groupby('unit_nr')['pca_1'].transform(lambda x: x.diff().fillna(0))
    
    joblib.dump(pca_scaler, ARTIFACT_DIR / "pca_scaler.pkl")
    joblib.dump(pca, ARTIFACT_DIR / "pca_model.pkl")
    
    # 2. MinMaxScaler ê°ì²´ ìƒì„± ë° ì €ì¥ (Fit)
    final_features = raw_sensors + ['pca_1', 'pca_1_trend']
    minmax_scaler = MinMaxScaler()
    minmax_scaler.fit(train_df[final_features])
    
    joblib.dump(minmax_scaler, ARTIFACT_DIR / "minmax_scaler.pkl")
    
    print(f"âœ… Preprocessors exported to {ARTIFACT_DIR}")

if __name__ == "__main__":
    export_top_models(top_n=3)
    export_preprocessors()
    print("ğŸ‰ All artifacts successfully packed for deployment!")