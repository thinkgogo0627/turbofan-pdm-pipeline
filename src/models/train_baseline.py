import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import ReduceLROnPlateau
import mlflow
import mlflow.pytorch
import mlflow.data 
from mlflow.data.pandas_dataset import PandasDataset 
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from numpy.lib.stride_tricks import sliding_window_view

PROJECT_DIR = Path(__file__).resolve().parents[2]

from src.features.schema import FeatureSchema
from src.models.model_zoo import DeepCNN, CNNAttention, TransformerModel, DLinear, Simple1DCNN
from src.models.model_config import TRAINER_CONFIG, MODEL_CONFIGS

def get_model(model_name, input_dim, model_conf):
    """ëª¨ë¸ Factory í•¨ìˆ˜ (ê¸°ì¡´ ë™ì¼)"""
    if model_name == "DLinear":
        return DLinear(seq_len=model_conf['window_size'], input_dim=input_dim)
    elif model_name == "Transformer":
        return TransformerModel(input_dim=input_dim, d_model=model_conf['d_model'], nhead=model_conf['nhead'])
    elif model_name == "DeepCNN":
        return DeepCNN(input_dim=input_dim, hidden_layers=model_conf['hidden_layers'], kernel_size=model_conf['kernel_size'], dropout=model_conf['dropout'])
    elif model_name == "CNNAttention":
        return CNNAttention(input_dim=input_dim, hidden_dim=model_conf['hidden_dim'])
    else:
        return Simple1DCNN(input_dim=input_dim)

def create_dataset(df, window_size, feature_cols):
    """DataFrame -> Windowed Numpy Array (ê¸°ì¡´ ë™ì¼)"""
    X_list, y_list = [], []
    
    # print(f"   [Info] Creating windows (Size: {window_size})...")
    
    for unit_nr, group in df.groupby('unit_nr'):
        data = group[feature_cols].values
        target = group['RUL'].values
        
        if len(data) < window_size:
            continue
            
        # Sliding Window
        windows = sliding_window_view(data, window_shape=window_size, axis=0)
        # Shape ë³€í™˜: (N, Window, Feat) -> ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì¡°ì • (N, Feat, Window)ê°€ í•„ìš”í•˜ë‹¤ë©´ transpose ìœ„ì¹˜ ì£¼ì˜
        # í˜„ì¬ DeepCNN ë“±ì€ (N, Feat, Window)ë¥¼ ê¸°ëŒ€í•˜ê±°ë‚˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•¨.
        # ì—¬ê¸°ì„œëŠ” (N, Window, Feat)ë¡œ ìœ ì§€í•˜ê³  ëª¨ë¸ ë‚´ë¶€ì—ì„œ transposeí•œë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜,
        # ê¸°ì¡´ ì½”ë“œëŒ€ë¡œ (0, 2, 1) Transposeë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        windows = windows.transpose(0, 2, 1) # (N, Feat, Window) í˜•íƒœ
        
        target_windows = target[window_size-1:]
        
        X_list.append(windows)
        y_list.append(target_windows)
        
    X = np.concatenate(X_list, axis=0)
    y = np.concatenate(y_list, axis=0)
    return X, y

def train_model(model_name):
    # 1. Config ë³‘í•©
    if model_name not in MODEL_CONFIGS:
        print(f"âš ï¸ Warning: No specific config for {model_name}. Using default.")
        model_specific_conf = {}
    else:
        model_specific_conf = MODEL_CONFIGS[model_name]

    full_config = TRAINER_CONFIG.copy()
    full_config.update(model_specific_conf)
    full_config['model_type'] = model_name
    
    # 2. MLflow ì„¤ì •
    current_time = datetime.now().strftime("%m%d_%H%M")
    run_name = f"{model_name}_{current_time}"
    mlflow.set_experiment("Turbofan_RUL_Prediction")

    with mlflow.start_run(run_name=run_name):
        print(f"ğŸš€ Start Training: {model_name}")
        print(f"ğŸ“œ Full Config: {full_config}")
        mlflow.log_params(full_config)

        # ----------------------------------------
        # 3. ë°ì´í„° ë¡œë“œ ë° ë¶„í•  (í•µì‹¬ ìˆ˜ì •!)
        # ----------------------------------------
        data_path = PROJECT_DIR / "data/processed/train_FD001_advanced_features.parquet"
        df = pd.read_parquet(data_path)


        ## RUL Clipping (ìµœëŒ€ 125ê¹Œì§€ë§Œ ì˜ˆì¸¡í•˜ë„ë¡ ì œí•œ)
        MAX_RUL = 125
        print(f" [Preprocessing] Clipping RUL to max {MAX_RUL}...")
        df['RUL'] = df['RUL'].clip(upper=MAX_RUL)
        
        # [Split Logic] Unit ID ê¸°ì¤€ ë¶„í•  (8:2)
        unit_ids = df['unit_nr'].unique()
        split_idx = int(len(unit_ids) * 0.8)
        train_units = unit_ids[:split_idx]
        val_units = unit_ids[split_idx:]
        
        print(f"   [Split] Train Units: {len(train_units)} / Val Units: {len(val_units)}")
        
        train_df = df[df['unit_nr'].isin(train_units)].copy()
        val_df = df[df['unit_nr'].isin(val_units)].copy()
        
        # MLflow Dataset Log (Train ê¸°ì¤€)
        dataset = mlflow.data.from_pandas(train_df, source=str(data_path), name="turbofan_train_split")
        mlflow.log_input(dataset, context="training")

        # ----------------------------------------
        # 4. Scaling (Leakage ë°©ì§€)
        # ----------------------------------------
        scaler = MinMaxScaler()
        feature_cols = full_config['features']
        
        # Trainìœ¼ë¡œ fit, Valì€ transformë§Œ!
        train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
        val_df[feature_cols] = scaler.transform(val_df[feature_cols])
        
        # ----------------------------------------
        # 5. Windowing & DataLoader
        # ----------------------------------------
        # Train Set
        X_train, y_train = create_dataset(train_df, full_config['window_size'], feature_cols)
        train_tensor = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).unsqueeze(1))
        train_loader = DataLoader(train_tensor, batch_size=full_config['batch_size'], shuffle=True)
        
        # Val Set
        X_val, y_val = create_dataset(val_df, full_config['window_size'], feature_cols)
        val_tensor = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32).unsqueeze(1))
        val_loader = DataLoader(val_tensor, batch_size=full_config['batch_size'], shuffle=False) # ì„ì§€ ì•ŠìŒ

        print(f"   [Data] Train Windows: {len(X_train)} / Val Windows: {len(X_val)}")

        # ----------------------------------------
        # 6. ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
        # ----------------------------------------
        model = get_model(model_name, len(feature_cols), full_config)
        
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=full_config['learning_rate'])
        patience = full_config.get('patience', 10) 
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience)

        # ----------------------------------------
        # 7. í•™ìŠµ ë£¨í”„ (Validation ì¶”ê°€)
        # ----------------------------------------
        print("ğŸ”¥ Training Loop Start...")
        for epoch in range(full_config['epochs']):
            # --- Training ---
            model.train()
            train_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            train_rmse = np.sqrt(avg_train_loss)

            # --- Validation ---
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            
            avg_val_loss = val_loss / len(val_loader)
            val_rmse = np.sqrt(avg_val_loss)
            
            # Scheduler Step (Validation ì ìˆ˜ ê¸°ì¤€)
            scheduler.step(avg_val_loss)
            
            # Logging
            if (epoch+1) % 10 == 0:
                lr = optimizer.param_groups[0]['lr']
                print(f"Epoch {epoch+1}/{full_config['epochs']} | Train RMSE: {train_rmse:.4f} | Val RMSE: {val_rmse:.4f} | LR: {lr:.6f}")
                
                mlflow.log_metric("train_rmse", train_rmse, step=epoch)
                mlflow.log_metric("val_rmse", val_rmse, step=epoch) # Val ì ìˆ˜ê°€ ì§„ì§œ ì¤‘ìš”í•¨!
                mlflow.log_metric("learning_rate", lr, step=epoch)

        # ëª¨ë¸ ì €ì¥
        mlflow.pytorch.log_model(model, "model")
        print(f"ğŸ‰ Training Finished. Final Val RMSE: {val_rmse:.4f}")

if __name__ == "__main__":
    train_model("DeepCNN")