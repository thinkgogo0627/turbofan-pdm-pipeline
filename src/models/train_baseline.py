import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import mlflow
import mlflow.data
from mlflow.data.pandas_dataset import PandasDataset
import mlflow.pytorch
from pathlib import Path
from datetime import datetime
import sys
from sklearn.preprocessing import MinMaxScaler

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ëª¨ë“ˆ importìš©)
PROJECT_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(PROJECT_DIR))

from src.features.schema import FeatureSchema # ë°©ê¸ˆ ë§Œë“  ìŠ¤í‚¤ë§ˆ
from src.models.model_zoo import DeepCNN, CNNAttention, TransformerModel, DLinear


# ==========================================
# 1. Config & Hyperparameters
# ==========================================
params = {
    "window_size": 30,    # ê³¼ê±° 30ì´ˆë¥¼ ë´„
    "batch_size": 64,
    "learning_rate": 0.001,
    "epochs": 10,
    "features": [
        'sensor_2_ema', 'sensor_3_ema', 'sensor_4_ema', 'sensor_7_ema',
        'sensor_11_ema', 'sensor_12_ema', 'sensor_15_ema' 
        # (Variation ì»¬ëŸ¼ë“¤ë„ ì¶”ê°€ ê°€ëŠ¥)
    ]
}

# ==========================================
# 2. Model Architecture (Simple 1D CNN)
# ==========================================
class Simple1DCNN(nn.Module):
    def __init__(self, input_dim):
        super(Simple1DCNN, self).__init__()
        # Conv1d: ì‹œê³„ì—´ì˜ 'ì§€ì—­ì  íŒ¨í„´'ì„ ì°¾ìŒ (í•„í„°ê°€ ì‹œê°„ì¶•ìœ¼ë¡œ ìŠ¬ë¼ì´ë”©)
        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=3)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.flatten = nn.Flatten()
        
        # Flatten í›„ì˜ ì°¨ì› ê³„ì‚°ì´ ê·€ì°®ìœ¼ë¯€ë¡œ AdaptiveAvgPool ì‚¬ìš© (ê¼¼ìˆ˜)
        # ì–´ë–¤ ê¸¸ì´ë“  1ê°œì˜ ê°’ìœ¼ë¡œ ì••ì¶•
        self.global_pool = nn.AdaptiveAvgPool1d(1) 
        self.fc = nn.Linear(32, 1) # RUL ì˜ˆì¸¡ (Regression)

    def forward(self, x):
        # x shape: (Batch, Time, Features) -> (Batch, Features, Time) ë³€í™˜ í•„ìš”
        x = x.transpose(1, 2)
        x = self.conv1(x)
        x = self.relu(x)
        x = self.global_pool(x) # (Batch, 32, 1)
        x = self.flatten(x)     # (Batch, 32)
        x = self.fc(x)          # (Batch, 1)
        return x

# ==========================================
# 3. Data Preparation (Sliding Window)
# ==========================================
def create_dataset(df, window_size, feature_cols):
    X_list, y_list = [], []
    
    # ì—”ì§„ë³„ë¡œ ìœˆë„ìš° ìë¥´ê¸°
    for unit_nr, group in df.groupby('unit_nr'):
        data = group[feature_cols].values
        target = group['RUL'].values
        
        # ë°ì´í„°ê°€ ìœˆë„ìš°ë³´ë‹¤ ì§§ìœ¼ë©´ íŒ¨ìŠ¤
        if len(data) < window_size: continue
            
        # Sliding Window (ì†ë„ ìµœì í™” ë²„ì „ ì•„ë‹˜, ì´í•´ìš©)
        for i in range(len(data) - window_size):
            X_list.append(data[i : i + window_size])
            y_list.append(target[i + window_size - 1]) # ìœˆë„ìš° ëì§€ì ì˜ RUL ì˜ˆì¸¡
            
    return np.array(X_list), np.array(y_list)

# ==========================================
# 4. Main Training Pipeline
# ==========================================
def train(model_type):

    # ì´ë¦„ ìƒì„±
    current_time = datetime.now().strftime("%m%d_%H%M")
    run_name = f"{model_type}_{current_time}"

    # MLflow ì‹¤í—˜ ì´ë¦„ ì„¤ì •
    mlflow.set_experiment("Turbofan_RUL_Prediction")

    
    with mlflow.start_run(run_name=run_name):
        # A. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦ (Pandera)
        print("[Step 1] Loading & Validating Data...")
        data_path = PROJECT_DIR / "data/processed/train_FD001_features.parquet"
        df = pd.read_parquet(data_path)

    
        ## MLflowì— ë°ì´í„°ì…‹ ì •ë³´ ë“±ë¡
        print("[Info] logging dataset info to mlflow")
        # pandas dataframe -> mlflow dataset ê°ì²´ ë³€í™˜
        dataset = mlflow.data.from_pandas(
            df,
            source=str(data_path),
            name = "turbofan_processed_data_ver_1"
        )
        # train ìš©ë„ë¡œ ì‚¬ìš©í–ˆë‹¤ê³  ê¸°ë¡
        mlflow.log_input(dataset, context="training")
        
        # Pandera ê²€ì¦ ìˆ˜í–‰ (ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬ ë°œìƒ)
        try:
            FeatureSchema.validate(df)
            print("âœ… Data Schema Validation Passed!")
        except Exception as e:
            print(f"âŒ Data Validation Failed: {e}")
            return

        ### Pandera ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í›„ Scaling ìˆ˜í–‰
        print("[Step 1.5] Applying MinMaxScaler")

        # Feature ì»¬ëŸ¼ , Target ì»¬ëŸ¼ ë¶„ë¦¬
        feature_cols = params['features']

        # ìŠ¤ì¼€ì¼ëŸ¬ ì •ì˜
        scaler = MinMaxScaler()

        # ë°ì´í„°í”„ë ˆì„ì˜ Feature ë§Œ ìŠ¤ì¼€ì¼ë§ -> Targetì€ ìŠ¤ì¼€ì¼ë§ X
        df[feature_cols] = scaler.fit_transform(df[feature_cols])


        # B. ì „ì²˜ë¦¬ (Windowing)
        print("[Step 2] Creating Sliding Windows...")
        X, y = create_dataset(df, params['window_size'], params['features'])
        
        # Tensor ë³€í™˜
        X_tensor = torch.tensor(X, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)
        
        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True)
        
        # C. ëª¨ë¸ ì´ˆê¸°í™”
        if model_type == "DeepCNN":
            model = DeepCNN(input_dim=len(params['features']))
        
        elif model_type == "CNNAttention":
            model = CNNAttention(input_dim=len(params['features']))
        
        elif model_type == "Transformer":
            model = TransformerModel(input_dim=len(params['features']))

        elif model_type == "DLinear":
            model = DLinear(seq_len=params['window_size'], input_dim=len(params['features']))
        
        else:
            model = Simple1DCNN(input_dim=len(params['features'])) # Default

        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])
        
        # MLflow íŒŒë¼ë¯¸í„° ê¸°ë¡
        mlflow.log_params(params)
        
        # D. í•™ìŠµ ë£¨í”„
        print("[Step 3] Training Start...")
        model.train()
        for epoch in range(params['epochs']):
            epoch_loss = 0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(dataloader)
            rmse = np.sqrt(avg_loss)
            print(f"Epoch {epoch+1}/{params['epochs']}, Loss: {avg_loss:.4f}, RMSE: {rmse:.4f}")
            
            # MLflow ë©”íŠ¸ë¦­ ê¸°ë¡
            mlflow.log_metric("rmse", rmse, step=epoch)
            
        # E. ëª¨ë¸ ì €ì¥
        print("[Step 4] Saving Model...")
        mlflow.pytorch.log_model(model, "model")
        print("ğŸ‰ Training Complete! Check MLflow UI.")

if __name__ == "__main__":
    train(model_type="DLinear")